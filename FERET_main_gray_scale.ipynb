{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline\n",
    "\n",
    "import numpy as np\n",
    "import random\n",
    "from feret_utils import get_feret_files_and_tags_dict, run_face_detection, create_dataset, create_dataset_gs\n",
    "from matplotlib.pyplot import imshow\n",
    "from IPython.display import Image\n",
    "import cv2 as cv\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Start by getting a dictionary holding file names of images as keys and id of person as value\n",
    "feret_dict = get_feret_files_and_tags_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# During training we want to see the affect of not using the \"pr\" and \"pl\" examples\n",
    "list_of_excluded = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a datast, using only images which were detected in run_face_detection run, and crop the images to new size\n",
    "subject_list, data_dict, mean_image, std_image = create_dataset_gs(feret_dict, subject_count=700, flip=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "# Save data_dict so we can unpickle it when needed\n",
    "data_to_pickle = (subject_list, data_dict, mean_image, std_image)\n",
    "with open('feret_data_dict_gs.pickle', 'wb') as handle:\n",
    "    pickle.dump(data_to_pickle, handle, protocol=pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "# Unpickle data_dict\n",
    "with open('feret_data_dict_gs.pickle', 'rb') as handle:\n",
    "    subject_list, data_dict, mean_image, std_image = pickle.load(handle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# List of images to exclude after manually going over dataset and deleting irrelevant pictures\n",
    "ex_list = [119, 120, 167, 168, 193, 194, 253, 254, 341, 342, 515, 516, 547, 548, 669, 670, 775, 776, 1171, 1172,\n",
    " 1181, 1182, 1213, 1214, 1275, 1276, 1395, 1396, 1517, 1518, 1521, 1522, 1531, 1532, 1613, 1614, 1685, 1686,\n",
    " 1695, 1696, 1701, 1702, 1703, 1704, 1725, 1726, 1747, 1748, 1779, 1780, 1789, 1790, 1899, 1900, 1937, 1938,\n",
    " 1945, 1946, 1951, 1952, 1953, 1954, 1955, 1956, 1987, 1988, 2093, 2094, 2107, 2108, 2115, 2116, 2125, 2126,\n",
    " 2163, 2164, 2181, 2182, 2199, 2200, 2213, 2214, 2229, 2230, 2255, 2256, 2281, 2281, 2283, 2284, 2311, 2312,\n",
    " 2333, 2334, 2381, 2382, 2397, 2398, 2411, 2412, 2497, 2498, 2519, 2520, 2673, 2674, 2683, 2684, 2743, 2744,\n",
    " 2761, 2762, 2777, 2778, 2833, 2834, 2857, 2858, 2873, 2874, 2875, 2876, 2919, 2920, 2939, 2940, 3021, 3022,\n",
    " 3067, 3068, 3105, 3106, 3143, 3144, 3183, 3184, 3213, 3214, 3227, 3228, 3233, 3234, 3245, 3246, 3251, 3252,\n",
    " 3252, 3254, 3267, 3268, 3353, 3354, 3401, 3402, 3405, 3406, 3407, 3408, 3417, 3418, 3447, 3448, 3717, 3718,\n",
    " 3741, 3742, 3759, 3760, 3813, 3814, 3863, 3864, 3865, 3866, 3871, 3872, 3875, 3876, 3883, 3884, 3913, 3914,\n",
    " 3943, 3944, 3963, 3964, 3967, 3968, 3973, 3981, 3982, 3999, 4000, 4013, 4014, 4095, 4096, 4113, 4114, 4141,\n",
    " 4142, 4159, 4160, 4175, 4176, 4181, 4182, 4183, 4184, 4195, 4196, 4219, 4220, 4223, 4224, 4233, 4234, 4261,\n",
    " 4262, 4263, 4264, 4281, 4282, 4319, 4320, 4349, 4350, 4351, 4352, 4371, 4372, 4377, 4378, 4397, 4398, 4405,\n",
    " 4406, 4407, 4408, 4421, 4422, 4425, 4426, 4455, 4456, 4577, 4578, 4631, 4632, 4641, 4642, 4645, 4646, 4657,\n",
    " 4658, 4667, 4668, 4685, 4686 ,4757, 4758, 4787, 4788, 4805, 4806, 4881, 4882, 4897, 4898, 4905, 4906, 5009,\n",
    " 5010, 5021, 5022, 5049, 5050, 5059, 5060, 5087, 5088, 5139, 5140, 5147, 5148, 5237, 5238, 5247, 5248, 5261,\n",
    " 5262, 5273, 5274, 5283, 5284, 5297, 5298, 5353, 5354, 5355, 5356, 5379, 5380, 5405, 5406, 5429, 5430, 5431,\n",
    " 5432, 5551, 5552, 5561, 5562, 5649, 5650, 5653, 5654, 5675, 5676, 5687, 5688, 5703, 5704, 5729, 5730, 5735,\n",
    " 5736, 5741, 5742, 5749, 5750, 5819, 5820, 5823, 5824, 5833, 5834, 5895, 5896, 5945, 5946, 5985, 5986, 6077,\n",
    " 6078, 6123, 6124, 6141, 6142, 6221, 6222, 6321, 6322, 6341, 6342, 6351, 6352, 6371, 6372, 6433, 6434, 6531,\n",
    " 6532, 6535, 6536, 6545, 6546, 6553, 6554, 6677, 6678, 6679, 6680, 6681, 6682, 6683, 6684, 6693, 6694, 6759,\n",
    " 6760, 6789, 6790, 6807, 6808, 6873, 6874, 6875, 6876, 6893, 6894, 6909, 6910, 6939, 6940, 6993, 6994, 7029,\n",
    " 7030, 7037, 7038, 7087, 7088, 7103, 7104, 7121, 7122, 7123, 7124, 7129, 7130, 7251, 7252, 7271, 7272, 7277,\n",
    " 7278, 7281, 7282]\n",
    "\n",
    "\n",
    "for i in reversed(ex_list):\n",
    "    data_dict['X_train'] = np.delete(data_dict['X_train'], i-1, axis=0)\n",
    "    data_dict['y_train'] = np.delete(data_dict['y_train'], i-1, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(6931, 96, 96, 1)\n",
      "(6931,)\n",
      "(268, 96, 96, 1)\n",
      "(268,)\n"
     ]
    }
   ],
   "source": [
    "print(data_dict['X_train'].shape)\n",
    "print(data_dict['y_train'].shape)\n",
    "print(data_dict['X_eval'].shape)\n",
    "print(data_dict['y_eval'].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.logging.set_verbosity(tf.logging.ERROR)\n",
    "tf.reset_default_graph()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "num_of_classes = len(subject_list)\n",
    "def cnn_model_fn(features, labels, mode):\n",
    "    \"\"\"CNN model function\"\"\"\n",
    "    \n",
    "    input_layer = tf.reshape(features[\"x\"], [-1, 96, 96, 1])\n",
    "    conv1 = tf.layers.conv2d(inputs=input_layer,\n",
    "                            filters=32,\n",
    "                            kernel_size=7,\n",
    "                            padding=\"same\",\n",
    "                            kernel_initializer=tf.contrib.layers.xavier_initializer_conv2d(),\n",
    "                            activation=tf.nn.leaky_relu)\n",
    "    pool1 = tf.layers.max_pooling2d(inputs=conv1, pool_size=2, strides =2)\n",
    "    conv2 = tf.layers.conv2d(inputs=pool1,\n",
    "                            filters=32,\n",
    "                            kernel_size=5,\n",
    "                            padding=\"same\",\n",
    "                            kernel_initializer=tf.contrib.layers.xavier_initializer_conv2d(),\n",
    "                            activation=tf.nn.leaky_relu)\n",
    "    pool2 = tf.layers.max_pooling2d(inputs=conv1, pool_size=2, strides =2)\n",
    "    conv3 = tf.layers.conv2d(inputs=pool2,\n",
    "                            filters=32,\n",
    "                            kernel_size=3,\n",
    "                            padding=\"same\",\n",
    "                            kernel_initializer=tf.contrib.layers.xavier_initializer_conv2d(),\n",
    "                            activation=tf.nn.leaky_relu)\n",
    "    pool3 = tf.layers.max_pooling2d(inputs=conv3, pool_size=2, strides =2)\n",
    "    pool3_flat = tf.reshape(pool3, [-1, 24 * 24 * 32])\n",
    "    dense = tf.layers.dense(inputs=pool3_flat, units=24 * 24 * 32, activation=tf.nn.relu)\n",
    "    dropout = tf.layers.dropout(inputs=dense, rate=0.1, training=mode == tf.estimator.ModeKeys.TRAIN)\n",
    "    \n",
    "    logits = tf.layers.dense(inputs=dropout, units=num_of_classes)\n",
    "    classes = tf.as_string(tf.argmax(input=logits, axis=1, name=\"class\"))\n",
    "    predictions = {\n",
    "        \"classes\" : tf.argmax(input=logits, axis=1),\n",
    "        \"probabilities\" : tf.nn.softmax(logits, name=\"softmax_tensor\"),\n",
    "        \"dense\" : dense,\n",
    "        \"pool3\" : pool3_flat\n",
    "        }\n",
    "        \n",
    "    if mode == tf.estimator.ModeKeys.PREDICT:\n",
    "        export_outputs = {\"predictions\" : tf.estimator.export.PredictOutput(predictions)}\n",
    "        return tf.estimator.EstimatorSpec(mode=mode, predictions=predictions, export_outputs=export_outputs)\n",
    "    \n",
    "    onehot_labels = tf.one_hot(indices=tf.cast(labels, tf.int32), depth = num_of_classes)\n",
    "    loss = tf.losses.softmax_cross_entropy(onehot_labels=onehot_labels, logits=logits)\n",
    "    \n",
    "    if mode == tf.estimator.ModeKeys.TRAIN:\n",
    "        # This is wrong, need to fix\n",
    "        metrics = {\n",
    "            \"accuracy\" : tf.metrics.accuracy(labels=labels, predictions=predictions[\"classes\"])\n",
    "        }\n",
    "        tf.summary.scalar(\"accuracy\", metrics[\"accuracy\"][1])\n",
    "        merge_summary_op = tf.summary.merge_all\n",
    "        lr = 1e-4\n",
    "        step_rate = 10000\n",
    "        decay = 0.8\n",
    "        learning_rate = tf.train.exponential_decay(lr, global_step=tf.train.get_or_create_global_step(), decay_steps=step_rate, decay_rate=decay, staircase=True)\n",
    "        optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate)\n",
    "        train_op = optimizer.minimize(loss=loss, global_step=tf.train.get_global_step())\n",
    "        return tf.estimator.EstimatorSpec(mode=mode, loss=loss, train_op=train_op)\n",
    "    eval_metrics_op = {\"accuracy\" : tf.metrics.accuracy(labels=labels, predictions=predictions[\"classes\"])}\n",
    "    \n",
    "    return tf.estimator.EstimatorSpec(mode=mode, loss=loss, eval_metric_ops=eval_metrics_op)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "face_classifier = tf.estimator.Estimator(model_fn=cnn_model_fn, model_dir=\"train/train_gs_0/\")\n",
    "tensors_to_log = {\"probabilities\" : \"softmax_tensor\"}\n",
    "logging_hook = tf.train.LoggingTensorHook(tensors=tensors_to_log, every_n_iter=200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.estimator.estimator.Estimator at 0x7f3bb15076d8>"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_input_fn = tf.estimator.inputs.numpy_input_fn(x={\"x\" : data_dict['X_train']},\n",
    "                                                       y=data_dict['y_train'],\n",
    "                                                       batch_size=25,\n",
    "                                                       num_epochs=None,\n",
    "                                                       shuffle=True)\n",
    "face_classifier.train(input_fn=training_input_fn, steps=1000, hooks=[logging_hook])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'accuracy': 0.6268657, 'loss': 1.5606836, 'global_step': 1000}\n"
     ]
    }
   ],
   "source": [
    "eval_input_fn = tf.estimator.inputs.numpy_input_fn(x={\"x\" : data_dict['X_eval']},\n",
    "                                                   y=data_dict['y_eval'],\n",
    "                                                   num_epochs=1,\n",
    "                                                   shuffle=False)\n",
    "eval_results = face_classifier.evaluate(input_fn=eval_input_fn)\n",
    "print(eval_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'accuracy': 0.95729333, 'loss': 0.17117465, 'global_step': 1000}\n"
     ]
    }
   ],
   "source": [
    "eval_input_fn = tf.estimator.inputs.numpy_input_fn(x={\"x\" : data_dict['X_train']},\n",
    "                                                   y=data_dict['y_train'],\n",
    "                                                   num_epochs=1,\n",
    "                                                   shuffle=False)\n",
    "eval_results = face_classifier.evaluate(input_fn=eval_input_fn)\n",
    "print(eval_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6931\n"
     ]
    }
   ],
   "source": [
    "predict_input_fn = tf.estimator.inputs.numpy_input_fn(x={\"x\" : data_dict['X_train'][:,:,:,:]}, num_epochs=1, shuffle = False)\n",
    "predictions = list(face_classifier.predict(input_fn=predict_input_fn))\n",
    "print(len(predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Calling model_fn.\n",
      "INFO:tensorflow:Done calling model_fn.\n",
      "INFO:tensorflow:Graph was finalized.\n",
      "INFO:tensorflow:Restoring parameters from train/train_gs_2/model.ckpt-14476\n",
      "INFO:tensorflow:Running local_init_op.\n",
      "INFO:tensorflow:Done running local_init_op.\n",
      "INFO:tensorflow:Calling model_fn.\n",
      "INFO:tensorflow:Done calling model_fn.\n",
      "INFO:tensorflow:Graph was finalized.\n",
      "INFO:tensorflow:Restoring parameters from train/train_gs_2/model.ckpt-14476\n",
      "INFO:tensorflow:Running local_init_op.\n",
      "INFO:tensorflow:Done running local_init_op.\n",
      "INFO:tensorflow:Calling model_fn.\n",
      "INFO:tensorflow:Done calling model_fn.\n",
      "INFO:tensorflow:Graph was finalized.\n",
      "INFO:tensorflow:Restoring parameters from train/train_gs_2/model.ckpt-14476\n",
      "INFO:tensorflow:Running local_init_op.\n",
      "INFO:tensorflow:Done running local_init_op.\n",
      "INFO:tensorflow:Calling model_fn.\n",
      "INFO:tensorflow:Done calling model_fn.\n",
      "INFO:tensorflow:Graph was finalized.\n",
      "INFO:tensorflow:Restoring parameters from train/train_gs_2/model.ckpt-14476\n",
      "INFO:tensorflow:Running local_init_op.\n",
      "INFO:tensorflow:Done running local_init_op.\n",
      "INFO:tensorflow:Calling model_fn.\n",
      "INFO:tensorflow:Done calling model_fn.\n",
      "INFO:tensorflow:Graph was finalized.\n",
      "INFO:tensorflow:Restoring parameters from train/train_gs_2/model.ckpt-14476\n",
      "INFO:tensorflow:Running local_init_op.\n",
      "INFO:tensorflow:Done running local_init_op.\n",
      "INFO:tensorflow:Calling model_fn.\n",
      "INFO:tensorflow:Done calling model_fn.\n",
      "INFO:tensorflow:Graph was finalized.\n",
      "INFO:tensorflow:Restoring parameters from train/train_gs_2/model.ckpt-14476\n",
      "INFO:tensorflow:Running local_init_op.\n",
      "INFO:tensorflow:Done running local_init_op.\n",
      "INFO:tensorflow:Calling model_fn.\n",
      "INFO:tensorflow:Done calling model_fn.\n",
      "INFO:tensorflow:Graph was finalized.\n",
      "INFO:tensorflow:Restoring parameters from train/train_gs_2/model.ckpt-14476\n",
      "INFO:tensorflow:Running local_init_op.\n",
      "INFO:tensorflow:Done running local_init_op.\n",
      "INFO:tensorflow:Calling model_fn.\n",
      "INFO:tensorflow:Done calling model_fn.\n",
      "INFO:tensorflow:Graph was finalized.\n",
      "INFO:tensorflow:Restoring parameters from train/train_gs_2/model.ckpt-14476\n",
      "INFO:tensorflow:Running local_init_op.\n",
      "INFO:tensorflow:Done running local_init_op.\n"
     ]
    }
   ],
   "source": [
    "with open('known_faces/faces.pickle', 'rb') as handle:\n",
    "    known_pics_dict = pickle.load(handle)\n",
    "\n",
    "dense_dict = {}\n",
    "    \n",
    "for file, pic in known_pics_dict.items():\n",
    "    pic = (pic - mean_image)/std_image\n",
    "    predict_input_fn = tf.estimator.inputs.numpy_input_fn(x={\"x\" : pic}, num_epochs=1, shuffle = False)\n",
    "    prediction = list(face_classifier.predict(input_fn=predict_input_fn))[0]\n",
    "    \n",
    "    dense_dict[file] = prediction[\"dense\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original is shahar4.jpg and closest is michi2.jpg with score of 0.24735122919082642\n",
      "Original is michi4.jpg and closest is shahar4.jpg with score of 0.5207279324531555\n",
      "Original is shahar1.jpg and closest is shahar2.jpg with score of 0.6190715432167053\n",
      "Original is shahar2.jpg and closest is shahar5.jpg with score of 0.42375075817108154\n",
      "Original is michi5.jpg and closest is michi3.jpg with score of 0.6107949614524841\n",
      "Original is michi2.jpg and closest is michi3.jpg with score of 0.15404987335205078\n",
      "Original is shahar5.jpg and closest is shahar2.jpg with score of 0.42375075817108154\n",
      "Original is michi3.jpg and closest is michi2.jpg with score of 0.15404987335205078\n"
     ]
    }
   ],
   "source": [
    "for file, dense in dense_dict.items():\n",
    "    lowest = 1000\n",
    "    key_file = ''\n",
    "    for key in dense_dict.keys():\n",
    "        if key == file:\n",
    "            continue\n",
    "        curr = np.mean((dense - dense_dict[key])**2)\n",
    "        if curr < lowest:\n",
    "            lowest = curr\n",
    "            key_file = key\n",
    "    print(\"Original is {} and closest is {} with score of {}\".format(file, key_file, lowest))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Calling model_fn.\n",
      "INFO:tensorflow:Done calling model_fn.\n",
      "INFO:tensorflow:Signatures INCLUDED in export for Classify: None\n",
      "INFO:tensorflow:Signatures INCLUDED in export for Regress: None\n",
      "INFO:tensorflow:Signatures INCLUDED in export for Predict: ['predictions', 'serving_default']\n",
      "INFO:tensorflow:Restoring parameters from train/train_gs_2/model.ckpt-24676\n",
      "INFO:tensorflow:Assets added to graph.\n",
      "INFO:tensorflow:No assets to write.\n",
      "INFO:tensorflow:SavedModel written to: b\"trained/trained_0/temp-b'1532617597'/saved_model.pb\"\n"
     ]
    }
   ],
   "source": [
    "def serving_input_receiver_fn():\n",
    "    \"\"\"An input receiver that expects a serialized tf.Example.\"\"\"\n",
    "    serialized_tf_example = tf.placeholder(dtype=tf.string, shape=[None], name='input_tensors')\n",
    "    receiver_tensors = {\"predictor_inputs\" : serialized_tf_example}\n",
    "    feature_spec = {\"x\" : tf.FixedLenFeature([96, 96 ,1], tf.float32)}\n",
    "    features = tf.parse_example(serialized_tf_example, feature_spec)\n",
    "    return tf.estimator.export.ServingInputReceiver(features, receiver_tensors)\n",
    "\n",
    "exported_model = face_classifier.export_savedmodel(\"trained/trained_0\", serving_input_receiver_fn=serving_input_receiver_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "103.659775 52.517044\n"
     ]
    }
   ],
   "source": [
    "print(mean_image, std_image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
