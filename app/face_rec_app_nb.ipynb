{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from feret_utils import extract_faces_gs\n",
    "import numpy as np\n",
    "import cv2\n",
    "import tensorflow as tf\n",
    "import pickle\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from feret/trained/trained_0/1532617597/variables/variables\n"
     ]
    }
   ],
   "source": [
    "# Get the tensorflow model\n",
    "tf.logging.set_verbosity(tf.logging.INFO)\n",
    "predict_fn = tf.contrib.predictor.from_saved_model(\"feret/trained/trained_0/1532617597\")\n",
    "mean_image, std_image = 103.659775, 52.517044"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('known_faces/faces.pickle', 'rb') as handle:\n",
    "    known_pics_dict = pickle.load(handle)\n",
    "    \n",
    "dense_dict = {}\n",
    "\n",
    "for file, pic in known_pics_dict.items():\n",
    "    for i, letter in enumerate(file):\n",
    "        if not letter.isalpha():\n",
    "            break\n",
    "    name = file[:i]\n",
    "    pic = (pic - mean_image)/std_image\n",
    "    X = pic.reshape(96 * 96 * 1).flatten().tolist()\n",
    "    model_input = tf.train.Example(features= tf.train.Features(feature={'x': tf.train.Feature(\n",
    "                                    float_list=tf.train.FloatList(value=X))}))\n",
    "    model_input = model_input.SerializeToString()\n",
    "    output_dict = predict_fn({\"predictor_inputs\":[model_input]})\n",
    "    dense_dict.setdefault(name, []).append(output_dict['dense'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "# Save output_dict so we can unpickle it when needed\n",
    "data_to_pickle = dense_dict\n",
    "with open('known_faces/dense_dict.pickle', 'wb') as handle:\n",
    "    pickle.dump(data_to_pickle, handle, protocol=pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "# Unpickle data_dict\n",
    "with open('known_faces/dense_dict.pickle', 'rb') as handle:\n",
    "    dense_dict = pickle.load(handle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_if_matched(img_dense, dense_dict):\n",
    "    scores_dict = {}\n",
    "    for name_itr in dense_dict.keys():   \n",
    "        sum_score, score = 0, 0\n",
    "        for dense_item in dense_dict[name_itr]:\n",
    "            sum_score += np.mean((np.abs(img_dense - dense_item))**3)\n",
    "        score = sum_score/len(dense_dict[name_itr])\n",
    "        scores_dict[name_itr] = score\n",
    "    lowest_name = min(scores_dict, key=scores_dict.get)\n",
    "    return(lowest_name, scores_dict[lowest_name])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_dense_for_pic(img):\n",
    "    img = (img - mean_image)/std_image\n",
    "    X = img.reshape(96 * 96 * 1).flatten().tolist()\n",
    "    model_input = tf.train.Example(features= tf.train.Features(feature={'x': tf.train.Feature(\n",
    "                                    float_list=tf.train.FloatList(value=X))}))\n",
    "    model_input = model_input.SerializeToString()\n",
    "    dense_out = predict_fn({\"predictor_inputs\":[model_input]})\n",
    "    \n",
    "    return dense_out[\"dense\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.7872286319732666\n",
      "0.5327664136886596\n",
      "0.31440134048461915\n",
      "0.8068335294723511\n",
      "0.6208767652511596\n",
      "0.6599600434303283\n",
      "0.6654536008834839\n",
      "0.544695895910263\n",
      "0.8334670662879944\n",
      "0.4289808630943298\n",
      "0.4723049521446228\n",
      "0.4445061504840851\n",
      "7.865639019012451\n",
      "7.172409152984619\n",
      "0.5251597881317138\n",
      "10.92102108001709\n",
      "10.685856437683105\n",
      "10.773926734924316\n",
      "10.854878807067871\n",
      "11.849378204345703\n",
      "0.957095491886139\n",
      "10.932256317138672\n",
      "0.7191604971885681\n",
      "12.083811187744141\n",
      "11.031088066101074\n",
      "10.922796630859375\n",
      "11.094811820983887\n",
      "10.554804229736328\n",
      "0.48938660621643065\n",
      "0.4351273238658905\n",
      "0.45414906144142153\n",
      "0.3961962997913361\n",
      "0.46531933546066284\n",
      "0.4254010558128357\n",
      "0.437485271692276\n",
      "0.40583178400993347\n",
      "0.42464346885681153\n",
      "0.40053836107254026\n",
      "0.40163968205451966\n",
      "0.4521890044212341\n",
      "0.40141727328300475\n",
      "0.4472260355949402\n",
      "0.45241097211837766\n",
      "0.4532576322555542\n",
      "0.47786529660224913\n",
      "0.5329801857471466\n",
      "0.5926781892776489\n",
      "10.91340446472168\n",
      "0.8396038055419922\n",
      "11.748898887634278\n",
      "11.330296325683594\n",
      "11.425756072998047\n",
      "9.555648803710938\n",
      "24.92394027709961\n",
      "13.173428344726563\n",
      "9.997001266479492\n",
      "1.179796290397644\n",
      "0.7765727400779724\n",
      "0.5955764472484588\n",
      "0.45451276898384096\n",
      "0.48647398352622984\n",
      "0.521948266029358\n",
      "0.5331571161746979\n",
      "0.5347799599170685\n",
      "0.5010791420936584\n",
      "0.515600323677063\n",
      "0.5048671960830688\n",
      "1.1806448936462401\n",
      "0.6042409241199493\n",
      "0.6193150579929352\n",
      "0.7379604339599609\n",
      "0.6236164450645447\n",
      "0.6589196205139161\n",
      "0.5036147236824036\n",
      "0.465571129322052\n",
      "0.468726372718811\n",
      "0.4669016063213348\n",
      "0.4416309893131256\n",
      "0.45479575991630555\n",
      "0.4523284137248993\n",
      "0.45755863189697266\n",
      "0.44161338806152345\n",
      "0.5395354211330414\n",
      "9.691840553283692\n",
      "9.385328483581542\n",
      "4.312331056594848\n",
      "3.942278432846069\n",
      "9.35034122467041\n",
      "4.9214186668396\n",
      "6.807876014709473\n",
      "10.754958534240723\n",
      "4.444270324707031\n",
      "6.031844615936279\n",
      "3.3897643089294434\n",
      "5.345642185211181\n",
      "7.133224678039551\n",
      "0.7815764367580413\n",
      "0.6373502731323242\n",
      "1.2624819993972778\n",
      "0.5626340210437775\n",
      "0.6778230369091034\n",
      "0.7763297080993652\n",
      "1.615721344947815\n",
      "1.0026529669761657\n",
      "1.1501262784004211\n",
      "0.7584683358669281\n",
      "0.5346474528312684\n",
      "0.591463851928711\n",
      "0.7949494123458862\n",
      "0.6943455815315247\n",
      "0.7021773099899292\n",
      "0.832417619228363\n",
      "0.6373252749443055\n",
      "0.7738577246665954\n",
      "0.7700317740440369\n",
      "0.7310216307640076\n",
      "0.7299389719963074\n",
      "0.6798739790916443\n",
      "0.7172029852867127\n",
      "0.6993937611579895\n",
      "0.7032838225364685\n",
      "0.7571462392807007\n",
      "0.655392324924469\n",
      "0.7613700866699219\n",
      "0.7129224538803101\n",
      "0.7499238729476929\n",
      "0.6790847420692444\n",
      "0.6656577706336975\n",
      "0.7022475361824035\n",
      "0.6096100807189941\n",
      "0.705388867855072\n",
      "0.72078857421875\n",
      "0.6006700992584229\n",
      "0.6207875847816468\n",
      "0.4957140624523163\n",
      "0.6694284558296204\n",
      "0.5641926348209381\n",
      "0.5723126888275146\n",
      "0.7392829895019531\n",
      "0.7111417531967164\n",
      "0.7740023493766784\n",
      "0.7597484707832336\n",
      "0.7781716823577881\n",
      "0.8195928573608399\n",
      "0.8680000543594361\n",
      "0.8210548043251038\n",
      "0.5873267352581024\n",
      "0.5115506112575531\n",
      "0.6631573438644409\n",
      "0.6233732223510742\n",
      "0.6659839272499084\n",
      "0.6927744626998902\n",
      "0.59133380651474\n",
      "0.7325170636177063\n",
      "2.0989218235015867\n",
      "1.5360384941101075\n",
      "0.6232714176177978\n",
      "0.48673161268234255\n",
      "0.6348036050796508\n"
     ]
    }
   ],
   "source": [
    "cap = cv2.VideoCapture(0)\n",
    "\n",
    "face_cascade = cv2.CascadeClassifier('/usr/local/share/OpenCV/haarcascades/haarcascade_frontalface_default.xml')\n",
    "found_time = 0\n",
    "unknown_count = 0\n",
    "while(True):\n",
    "    # Capture frame-by-frame\n",
    "    ret, frame = cap.read()\n",
    "\n",
    "    # Our operations on the frame come here\n",
    "    gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
    "    \n",
    "    faces = face_cascade.detectMultiScale(\n",
    "        gray,\n",
    "        scaleFactor=1.09,\n",
    "        minNeighbors=3,\n",
    "        minSize=(96, 96)\n",
    "    )\n",
    "    if (len(faces) > 0):\n",
    "        if time.time() - found_time > 1:\n",
    "            positions = {}\n",
    "            for (x, y, w, h) in faces:         \n",
    "                crop_img = gray[y:y+h, x:x+w]\n",
    "                crop_img = cv2.resize(crop_img, dsize=(96, 96), interpolation=cv2.INTER_NEAREST)\n",
    "                dense_for_pic = create_dense_for_pic(crop_img)\n",
    "\n",
    "                # Do predicition\n",
    "                name, score = check_if_matched(dense_for_pic, dense_dict)\n",
    "                print(score)\n",
    "                if (score > 1):\n",
    "                    name = (\"Unknown\")\n",
    "                \n",
    "                # Keep position data to continue drawing square when there is no prediction\n",
    "                positions[(x, y, w, h)] = name\n",
    " \n",
    "                cv2.rectangle(gray, (x, y), (x+w, y+h), (0, 255, 0), 2)\n",
    "                cv2.putText(gray, name, (x, y-10), cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 255, 0), 2)\n",
    "\n",
    "                found_time = time.time()\n",
    "        else:\n",
    "            for pos, name in positions.items():\n",
    "                x, y, w, h = pos\n",
    "                cv2.rectangle(gray, (x, y), (x+w, y+h), (0, 255, 0), 2)\n",
    "                cv2.putText(gray, name, (x, y-10), cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 255, 0), 2)\n",
    "    \n",
    "    # Display the resulting frame\n",
    "    cv2.imshow('frame',gray)\n",
    "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "        break\n",
    "\n",
    "# When everything done, release the capture\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "11\n",
      "12\n",
      "13\n",
      "14\n",
      "15\n",
      "16\n",
      "17\n",
      "18\n",
      "19\n",
      "20\n",
      "21\n",
      "22\n",
      "23\n",
      "24\n",
      "25\n",
      "26\n",
      "27\n",
      "28\n",
      "29\n",
      "30\n",
      "31\n",
      "32\n",
      "33\n",
      "34\n",
      "35\n",
      "36\n",
      "37\n",
      "38\n",
      "39\n",
      "40\n",
      "41\n",
      "42\n",
      "43\n",
      "44\n",
      "45\n",
      "46\n",
      "47\n",
      "48\n",
      "49\n",
      "50\n",
      "51\n"
     ]
    }
   ],
   "source": [
    "cap = cv2.VideoCapture(0)\n",
    "face_cascade = cv2.CascadeClassifier('/usr/local/share/OpenCV/haarcascades/haarcascade_frontalface_default.xml')\n",
    "i = 0\n",
    "while(True):\n",
    "\n",
    "    # Capture frame-by-frame\n",
    "    ret, frame = cap.read()\n",
    "    i += 1\n",
    "    # Our operations on the frame come here\n",
    "    gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
    "    \n",
    "    faces = face_cascade.detectMultiScale(\n",
    "        gray,\n",
    "        scaleFactor=1.09,\n",
    "        minNeighbors=3,\n",
    "        minSize=(96, 96)\n",
    "    )\n",
    "    print(i)\n",
    "    if (len(faces) == 1) and (i % 10 == 0):\n",
    "        (x, y, w, h) = faces[0]\n",
    "            \n",
    "        crop_img = gray[y:y+h, x:x+w]\n",
    "        crop_img = cv2.resize(crop_img, dsize=(96, 96), interpolation=cv2.INTER_NEAREST)\n",
    "        cv2.imwrite(('samples/shahar_%s.jpg' % i),crop_img)\n",
    "        \n",
    "        \n",
    "    \n",
    "    # Display the resulting frame\n",
    "    cv2.imshow('frame',gray)\n",
    "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "        break\n",
    "    if i == 51:\n",
    "        break\n",
    "    \n",
    "cap.release()\n",
    "cv2.destroyAllWindows() "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
